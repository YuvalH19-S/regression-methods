{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuval\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from functools import wraps\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from countryinfo import CountryInfo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Build Class Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_shapes(X, y):\n",
    "    # if data is a pandas dataframe, we convert it to a numpy array\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    if y.ndim != 1:\n",
    "        y = y.reshape(-1)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise Exception('X and y have different sizes.')\n",
    "    return X, y\n",
    "\n",
    "# build class for Simple Linear Regression\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.coeff = None\n",
    "        self.intercept = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # We validate the shapes of X and y\n",
    "        X,y = validate_shapes(X, y)  \n",
    "            \n",
    "        # We add a column of 1s which will be used to calculate the intercept\n",
    "        ones = np.ones(shape=X.shape[0]).reshape(-1, 1)\n",
    "        X = np.concatenate((ones, X), 1)\n",
    "\n",
    "        # We calculate the coefficients using the normal equation\n",
    "        X_transpose = X.transpose()\n",
    "        self.coeff = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n",
    "\n",
    "        # The first coefficient is the intercept, the rest of the coefficients are the beta values\n",
    "        self.intercept = self.coeff[0]\n",
    "        self.coeff = self.coeff[1:]\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.coeff is None or self.intercept is None:\n",
    "            raise Exception('Model is not fitted yet. Run fit method before prediction.')\n",
    "        \n",
    "        return X.dot(self.coeff) + self.intercept\n",
    "    \n",
    "class myLogisticRegression:\n",
    "\n",
    "    def __init__(self, lr = 0.01, num_iter = 100000, random_state = 2023):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.weights_ = None\n",
    "        self.bias_ = None\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"lr\": self.lr, \"num_iter\": self.num_iter, \"random_state\": self.random_state}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights_ = np.zeros(n_features)\n",
    "        self.bias_ = 0\n",
    "        \n",
    "        for _ in range(self.num_iter):\n",
    "            linear_model = np.dot(X, self.weights_) + self.bias_\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "            \n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            self.weights_ -= self.lr * dw\n",
    "            self.bias_ -= self.lr * db\n",
    "            \n",
    "        return self  \n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        linear_model = np.dot(X, self.weights_) + self.bias_\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = np.array([1 if i > 0.5 else 0 for i in y_predicted])\n",
    "        return y_predicted_cls\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        linear_model = np.dot(X, self.weights_) + self.bias_\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        \n",
    "        # Compute the complement probabilities, i.e., 1 - p\n",
    "        complement_proba = 1 - y_predicted\n",
    "\n",
    "        # Stack together for each sample\n",
    "        return np.vstack((complement_proba, y_predicted)).T\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "            \n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "         \n",
    "class PCA:\n",
    "    def __init__(self, n_components, eps=1e-10):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.eigenvalues = None\n",
    "        self.eps = eps  # small constant value to avoid division by zero\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Scaling the data\n",
    "        X = X - np.mean(X, axis=0)\n",
    "        X = X / (np.std(X, axis=0) + self.eps)  # add small constant in the denominator\n",
    "    \n",
    "        # Compute covariance\n",
    "        cov = np.cov(X.T)\n",
    "        cov = (cov + cov.T) / 2 # Symmetric matrix\n",
    "\n",
    "        # Eigenvalues, eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "\n",
    "        # Sort eigenvectors\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "\n",
    "        # Store first n eigenvectors\n",
    "        self.components = eigenvectors[0:self.n_components]\n",
    "        self.eigenvalues = eigenvalues\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X - np.mean(X, axis=0)\n",
    "        X = X / (np.std(X, axis=0) + self.eps) # Scaling the data\n",
    "        return np.dot(X, self.components.T)\n",
    "\n",
    "\n",
    "    def plot_scree(self, n_components_plot=None):\n",
    "        # Calculate the percentage of variance each component accounts for\n",
    "        var_percentages = self.eigenvalues / sum(self.eigenvalues)\n",
    "\n",
    "        # Cumulative variance percentages (summing upward)\n",
    "        var_percentages_cumulative = np.cumsum(var_percentages)\n",
    "\n",
    "        if n_components_plot is not None:\n",
    "            var_percentages = var_percentages[:n_components_plot]\n",
    "            var_percentages_cumulative = var_percentages_cumulative[:n_components_plot]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(1, len(var_percentages) + 1), var_percentages)\n",
    "        plt.plot(range(1, len(var_percentages_cumulative) + 1), var_percentages_cumulative, color='r', marker='o')\n",
    "\n",
    "        # Adding the variance percentage on top of each bar\n",
    "        for i, bar in enumerate(bars):\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval*100, 2), ha='center', va='bottom')\n",
    "\n",
    "        plt.xlabel('Principal Components')\n",
    "        plt.ylabel('Percentage of Variance Explained')\n",
    "        plt.title('Scree Plot')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Models - Compare to sklearn models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaN values:  6759\n",
      "Number of rows after removing NaN values:  5032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>host_total_listings_count</th>\n",
       "      <th>host_verifications</th>\n",
       "      <th>...</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>calculated_host_listings_count_entire_homes</th>\n",
       "      <th>calculated_host_listings_count_private_rooms</th>\n",
       "      <th>calculated_host_listings_count_shared_rooms</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>expensive</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>distance_from_capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>964081</td>\n",
       "      <td>13/08/2011</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>87%</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1</td>\n",
       "      <td>Sumida</td>\n",
       "      <td>Japan</td>\n",
       "      <td>7.357574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>801494</td>\n",
       "      <td>10/07/2011</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>...</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0</td>\n",
       "      <td>Kita</td>\n",
       "      <td>Japan</td>\n",
       "      <td>6.428651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5596383</td>\n",
       "      <td>24/03/2013</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1</td>\n",
       "      <td>Setagaya</td>\n",
       "      <td>Japan</td>\n",
       "      <td>8.229973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6809418</td>\n",
       "      <td>09/06/2013</td>\n",
       "      <td>within a day</td>\n",
       "      <td>100%</td>\n",
       "      <td>77%</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1</td>\n",
       "      <td>Shibuya</td>\n",
       "      <td>Japan</td>\n",
       "      <td>4.671359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>38655722</td>\n",
       "      <td>16/07/2015</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>99%</td>\n",
       "      <td>f</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>...</td>\n",
       "      <td>t</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>Suginami</td>\n",
       "      <td>Japan</td>\n",
       "      <td>11.820971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   host_id  host_since  host_response_time host_response_rate   \n",
       "0   1    964081  13/08/2011  within a few hours               100%  \\\n",
       "1   2    801494  10/07/2011      within an hour               100%   \n",
       "2   3   5596383  24/03/2013      within an hour               100%   \n",
       "4   5   6809418  09/06/2013        within a day               100%   \n",
       "6   7  38655722  16/07/2015      within an hour               100%   \n",
       "\n",
       "  host_acceptance_rate host_is_superhost  host_listings_count   \n",
       "0                  87%                 f                    1  \\\n",
       "1                 100%                 f                    1   \n",
       "2                 100%                 f                    1   \n",
       "4                  77%                 t                    1   \n",
       "6                  99%                 f                    9   \n",
       "\n",
       "   host_total_listings_count  host_verifications  ... instant_bookable   \n",
       "0                          2  ['email', 'phone']  ...                f  \\\n",
       "1                          6  ['email', 'phone']  ...                t   \n",
       "2                          4  ['email', 'phone']  ...                f   \n",
       "4                          2  ['email', 'phone']  ...                f   \n",
       "6                          9  ['email', 'phone']  ...                t   \n",
       "\n",
       "  calculated_host_listings_count  calculated_host_listings_count_entire_homes   \n",
       "0                              1                                            1  \\\n",
       "1                              1                                            0   \n",
       "2                              1                                            0   \n",
       "4                              1                                            1   \n",
       "6                              9                                            0   \n",
       "\n",
       "   calculated_host_listings_count_private_rooms   \n",
       "0                                             0  \\\n",
       "1                                             1   \n",
       "2                                             1   \n",
       "4                                             0   \n",
       "6                                             5   \n",
       "\n",
       "  calculated_host_listings_count_shared_rooms reviews_per_month  expensive   \n",
       "0                                           0              1.22          1  \\\n",
       "1                                           0              1.87          0   \n",
       "2                                           0              1.89          1   \n",
       "4                                           0              0.97          1   \n",
       "6                                           0              0.34          0   \n",
       "\n",
       "       city  country  distance_from_capital  \n",
       "0    Sumida    Japan               7.357574  \n",
       "1      Kita    Japan               6.428651  \n",
       "2  Setagaya    Japan               8.229973  \n",
       "4   Shibuya    Japan               4.671359  \n",
       "6  Suginami    Japan              11.820971  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# remove NaN values\n",
    "print(\"Number of rows before removing NaN values: \",len(train_data))\n",
    "train_data = train_data.dropna()\n",
    "# number of rows before removing NaN values\n",
    "print(\"Number of rows after removing NaN values: \",len(train_data))\n",
    "train_data.head()\n",
    "\n",
    "numerical_features_train = train_data.select_dtypes(include=['int64','float64']).columns\n",
    "# target - review_scores_value \n",
    "target = 'review_scores_value'\n",
    "# features - all numerical features\n",
    "X = train_data[numerical_features_train].drop(['expensive','id','host_id','review_scores_value'],axis=1)\n",
    "# delete index column \n",
    "X = X.drop(X.columns[0],axis=1)\n",
    "\n",
    "X.head()\n",
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Simple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn implementation:\n",
      "y =  [0.47684139] * X +  2.4063498680272364\n",
      "my implementation:\n",
      "y =  [0.47684139] * X +  2.406349868027182\n",
      "check if results are the same:\n",
      "True\n",
      "results are the same\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compare between sklearn and my implementation of simple linear regression\n",
    "\n",
    "X = train_data['review_scores_location']\n",
    "y = train_data['review_scores_value']\n",
    "# set seed for reproducibility\n",
    "np.random.seed(2023)\n",
    "\n",
    "print('sklearn implementation:')\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X.values.reshape(-1,1),y)\n",
    "print('y = ',model.coef_,'* X + ',model.intercept_)\n",
    "\n",
    "print('my implementation:')\n",
    "mymodel = SimpleLinearRegression()\n",
    "mymodel.fit(X.values,y.values)\n",
    "print('y = ',mymodel.coeff,'* X + ',mymodel.intercept)\n",
    "\n",
    "# check if results are the same \n",
    "print('check if results are the same:') # with tolerance of 1e-5\n",
    "equal = np.allclose(model.coef_,mymodel.coeff) and np.allclose(model.intercept_,mymodel.intercept)\n",
    "print(equal) \n",
    "print('results are the same') if equal else print('results are not the same')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target = 'expensive' \n",
    "X = train_data[numerical_features_train].drop(['expensive','id','host_id','review_scores_value'],axis=1)\n",
    "X = X.drop(X.columns[0],axis=1)\n",
    "X = train_data['review_scores_location']\n",
    "y = train_data[target] \n",
    "X ,y = X.values.reshape(-1,1),y.values \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "sklearn_model = LogisticRegression() \n",
    "sklearn_model.fit(X.reshape(-1,1),y)\n",
    "sklearn_preds = sklearn_model.predict(X.reshape(-1,1))\n",
    "\n",
    "# my implementation\n",
    "\n",
    "my_model = myLogisticRegression()\n",
    "my_model.fit(X,y)\n",
    "my_preds = my_model.predict(X)\n",
    "\n",
    "# check if results are the same\n",
    "equal = np.allclose(sklearn_preds,my_preds)\n",
    "print(equal)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.8807555 ,  0.23934582],\n",
       "        [-1.54414094,  0.02039729],\n",
       "        [-2.36660038, -1.2214701 ],\n",
       "        ...,\n",
       "        [-0.85864947, -1.47247896],\n",
       "        [ 2.34436608, -1.47817326],\n",
       "        [ 1.51694081, -0.65551211]]),\n",
       " array([[-0.88074645,  0.23937822],\n",
       "        [-1.54416033,  0.02031999],\n",
       "        [-2.36658468, -1.22140891],\n",
       "        ...,\n",
       "        [-0.85865515, -1.47250379],\n",
       "        [ 2.34437704, -1.47813545],\n",
       "        [ 1.51696569, -0.65541675]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare between sklearn and my implementation of PCA\n",
    "from sklearn.decomposition import PCA as skPCA\n",
    "\n",
    "X = train_data[numerical_features_train].drop(['expensive','id','host_id','review_scores_value'],axis=1)\n",
    "X = X.drop(X.columns[0],axis=1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X) # need to scale the data in order to get the same results (my implemntation do it inside the PCA class)\n",
    "\n",
    "# Number of principal components\n",
    "n_components = 2  \n",
    "\n",
    "# My PCA\n",
    "my_pca = PCA(n_components=n_components)\n",
    "my_pca.fit(X)\n",
    "X_my_pca = my_pca.transform(X)\n",
    "\n",
    "# Sklearn PCA\n",
    "sk_pca = skPCA(n_components=n_components)\n",
    "X_sk_pca = sk_pca.fit_transform(X)\n",
    "\n",
    "equal = np.allclose(np.abs(X_my_pca), np.abs(X_sk_pca), atol=1e-3)\n",
    "print(equal)\n",
    "\n",
    "X_my_pca,X_sk_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are equal within the tolerance of '0.001' after taking absolute values, due to the arbitrary sign orientation of principal components in different PCA implementations, and the tolerance parameter accounts for minor numerical differences due to computational precision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Apply Best Model - Chosen from the report analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocess and feature engeneering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function \n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ---------------Helper functions-----------------#\n",
    "\n",
    "# define the power transformer\n",
    "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# define the column transformer for categorical variables\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "def fill_na_with_host_specific_values(data, host_cols):\n",
    "    \"\"\"\n",
    "    Fills the missing values in the host_cols with the mean (for numerical columns) or mode \n",
    "    (for categorical columns) specific to each host_id. If the host specific mean or mode \n",
    "    cannot be computed, it is filled with the general mean or mode of the entire dataset.\n",
    "    \"\"\"\n",
    "    # General mean and mode for each column\n",
    "    general_values = {\n",
    "        col: data[col].mean() if data[col].dtype in ['int64', 'float64'] else data[col].mode()[0] \n",
    "        for col in host_cols\n",
    "    }\n",
    "\n",
    "    # Find the host_ids that has na values for each col and fill with their mean or mode value \n",
    "    for col in host_cols:\n",
    "        # Find the host_ids that has na values for col\n",
    "        host_ids_na = data[data[col].isnull()]['host_id'].unique()\n",
    "\n",
    "        for host_id in host_ids_na:\n",
    "            # If column is numeric, fill with mean. Otherwise, fill with mode.\n",
    "            if data[col].dtype in ['int64', 'float64']:\n",
    "                mean_val = data[(data['host_id'] == host_id) & (data[col].notnull())][col].mean()\n",
    "                fill_val = mean_val if not np.isnan(mean_val) else general_values[col]\n",
    "            else:\n",
    "                mode_val = data[(data['host_id'] == host_id) & (data[col].notnull())][col].mode()\n",
    "                fill_val = mode_val[0] if not mode_val.empty else general_values[col]\n",
    "            \n",
    "            # Fill the missing values\n",
    "            data.loc[(data['host_id'] == host_id) & (data[col].isnull()), col] = fill_val\n",
    "\n",
    "    return data\n",
    "\n",
    "def cluster_text(text,plot=False):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english') #random_state=2023\n",
    "\n",
    "    \n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.cluster import KMeans\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(2,15)\n",
    "    for k in K:\n",
    "       km = KMeans(n_clusters=k, max_iter=200, n_init=10, random_state=2023)\n",
    "       km = km.fit(X)\n",
    "       Sum_of_squared_distances.append(km.inertia_)\n",
    "    if plot:\n",
    "        plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Sum_of_squared_distances')\n",
    "        plt.title('Elbow Method For Optimal k')\n",
    "        plt.show()\n",
    "    \n",
    "    true_k = 11 # chosen after seeing the elbow plot\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10, random_state=2023)\n",
    "    model.fit(X)\n",
    "\n",
    "    labels=model.labels_\n",
    "    # labels as object\n",
    "    labels = labels.astype('object')\n",
    "    clusters=pd.DataFrame(list(zip(text,labels)),columns=['title','cluster'])\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def get_best_k(X, max_clusters):\n",
    "    best_k = 2\n",
    "    best_score = -1\n",
    "    for i in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        preds = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, preds)\n",
    "        if score > best_score:\n",
    "            best_k = i\n",
    "            best_score = score\n",
    "    return best_k\n",
    "\n",
    "def manipulate_bathrooms_text(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'half' in value.lower():\n",
    "            return 0.5\n",
    "        elif any(char.isdigit() for char in value):\n",
    "            digits = ''.join(filter(lambda char: char.isdigit() or char == '.', value))\n",
    "            return float(digits)\n",
    "    return np.nan\n",
    "\n",
    "def impute_missing_values_mice_train_test(train_data, test_data, columns, random_state=2023, max_iter=20):\n",
    "    \"\"\"\n",
    "    Use IterativeImputer to impute missing values in train and test data.\n",
    "    \"\"\"\n",
    "    # Define imputer\n",
    "    imputer = IterativeImputer(random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    # Fit on the training dataset\n",
    "    imputer.fit(train_data.loc[:, columns])\n",
    "    \n",
    "    # Transform the training and test datasets\n",
    "    train_data[columns] = imputer.transform(train_data.loc[:, columns])\n",
    "    test_data[columns] = imputer.transform(test_data.loc[:, columns])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def create_new_date_features(data):\n",
    "    columns = data.columns\n",
    "    # encode 'host_since' to the number of years, months, days and hours\n",
    "    data['host_since_year'] = data['host_since'].dt.year\n",
    "    data['host_since_month'] = data['host_since'].dt.month\n",
    "    data['host_since_day'] = data['host_since'].dt.day\n",
    "    # calculathe number of days, months and years between last_review and first_review\n",
    "    data['days_between_last_first_review'] = (data['last_review'] - data['first_review']).dt.days\n",
    "    data['months_between_last_first_review'] = (data['last_review'] - data['first_review']).dt.days / 30\n",
    "    data['years_between_last_first_review'] = (data['last_review'] - data['first_review']).dt.days / 365\n",
    "\n",
    "    # ---------------# calculate number of days, months and years between now and last_review, first_review and now\n",
    "    # calculate number of days, months and years between now and last_review, first_review and now \n",
    "    data['days_between_now_last_review'] = (pd.to_datetime('now') - data['last_review']).dt.days\n",
    "    data['months_between_now_last_review'] = (pd.to_datetime('now') - data['last_review']).dt.days / 30\n",
    "    data['years_between_now_last_review'] = (pd.to_datetime('now') - data['last_review']).dt.days / 365\n",
    "\n",
    "\n",
    "    data['days_between_now_first_review'] = (pd.to_datetime('now') - data['first_review']).dt.days\n",
    "    data['months_between_now_first_review'] = (pd.to_datetime('now') - data['first_review']).dt.days / 30\n",
    "    data['years_between_now_first_review'] = (pd.to_datetime('now') - data['first_review']).dt.days / 365\n",
    "\n",
    "    # calculate number of days, months and years between now and last_review, first_review and host_since\n",
    "    data['days_between_host_since_last_review'] = (data['last_review'] - data['host_since']).dt.days\n",
    "    data['months_between_host_since_last_review'] = (data['last_review'] - data['host_since']).dt.days / 30\n",
    "    data['years_between_host_since_last_review'] = (data['last_review'] - data['host_since']).dt.days / 365\n",
    "    data['days_between_host_since_first_review'] = (data['first_review'] - data['host_since']).dt.days\n",
    "    data['months_between_host_since_first_review'] = (data['first_review'] - data['host_since']).dt.days / 30\n",
    "\n",
    "\n",
    "    # calculate number of days, months and years between now and host_since\n",
    "    data['days_between_now_host_since'] = (pd.to_datetime('now') - data['host_since']).dt.days\n",
    "    data['months_between_now_host_since'] = (pd.to_datetime('now') - data['host_since']).dt.days / 30\n",
    "    data['years_between_now_host_since'] = (pd.to_datetime('now') - data['host_since']).dt.days / 365\n",
    "    new_features_columns = [col for col in data.columns if col not in columns]\n",
    "    return data, new_features_columns\n",
    "\n",
    "def create_new_frequncy_features(data):\n",
    "    data = data.copy()\n",
    "    columns = data.columns\n",
    "    data['frequency_of_reviews'] = data['number_of_reviews'] / data['reviews_per_month']\n",
    "    #  Availability Ratio\n",
    "    data['availability_ratio'] = data['availability_365'] / 365\n",
    "\n",
    "    # Short-term Availability Ratio\n",
    "    data['short_term_availability_ratio'] = data['availability_30'] / 30\n",
    "\n",
    "    # Change in Availability\n",
    "    data['change_in_availability_30_60'] = data['availability_60'] - data['availability_30']\n",
    "    data['change_in_availability_60_90'] = data['availability_90'] - data['availability_60']\n",
    "    data['change_in_availability_90_30'] = data['availability_90'] - data['availability_30']\n",
    "    data['change_in_availability_90_365'] = data['availability_365'] - data['availability_90']\n",
    "    data['change_in_availability_30_365'] = data['availability_365'] - data['availability_30']\n",
    "    data['change_in_availability_60_365'] = data['availability_365'] - data['availability_60']\n",
    "    # Average Stay\n",
    "    data['average_stay'] = (data['minimum_nights'] + data['maximum_nights']) / 2\n",
    "\n",
    "    # Nights Range\n",
    "    data['nights_range'] = data['maximum_nights'] - data['minimum_nights']\n",
    "\n",
    "    # Difference between Min and Max Nights\n",
    "    data['diff_max_nights'] = data['maximum_maximum_nights'] - data['minimum_maximum_nights']\n",
    "    data['diff_min_nights'] = data['maximum_minimum_nights'] - data['minimum_minimum_nights']\n",
    "\n",
    "    # Average Nights\n",
    "    data['avg_nights'] = (data['minimum_nights_avg_ntm'] + data['maximum_nights_avg_ntm']) / 2\n",
    "    new_features_columns = [col for col in data.columns if col not in columns]\n",
    "    return data, new_features_columns\n",
    "\n",
    "num_rows = 6759\n",
    "counter = 0\n",
    "geolocator = Nominatim(user_agent=\"MyApp\")\n",
    "capital_cache = {}\n",
    "\n",
    "def get_location_by_coordinates(lat, lon):\n",
    "    global counter\n",
    "    location = geolocator.reverse([lat, lon], exactly_one=True,language='en')\n",
    "    address = location.raw['address']\n",
    "    city = address.get('city', '')\n",
    "    country = address.get('country', '')\n",
    "    capital = CountryInfo(country).capital()\n",
    "    \n",
    "    # Check if capital's coordinates exist in cache\n",
    "    if capital not in capital_cache:\n",
    "        capital_location = geolocator.geocode(capital)\n",
    "        capital_cache[capital] = (capital_location.latitude,capital_location.longitude)\n",
    "\n",
    "    distance_from_capital = distance.distance((lat, lon), capital_cache[capital]).km\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print(f'{counter} out of {num_rows} rows processed')\n",
    "    return city, country, distance_from_capital\n",
    "\n",
    "def create_new_features(data):\n",
    "    data, _ = create_new_date_features(data)\n",
    "    data, _ = create_new_frequncy_features(data)\n",
    "    license_list = data['license'].tolist()\n",
    "    data['license_cluster'] = cluster_text(license_list)['cluster'].astype('object')\n",
    "    data['license_cluster'] = data['license_cluster'].fillna(data['license_cluster'].mode()[0]) # fill missing values with mode\n",
    "    # Add cluster info to the original DataFrame\n",
    "    location_data = data[['latitude', 'longitude']]\n",
    "    kmeans = KMeans(n_clusters=get_best_k(location_data,10), init='k-means++', max_iter=300, n_init=10, random_state=2023)\n",
    "    pred_y = kmeans.fit_predict(location_data)\n",
    "    data['location_cluster'] = pred_y.astype('object')\n",
    "    data['bathrooms_num'] = data['bathrooms_text'].apply(manipulate_bathrooms_text)\n",
    "    data['bedrooms_per_accommodat'] = data['bathrooms_num'] / data['accommodates']\n",
    "    data['num_amenities'] = data['amenities'].str.split(',').apply(len)\n",
    "    data['occupancy_rate'] = (365 - data['availability_365'])/365\n",
    "    data['occupancy_rate'] = data['occupancy_rate'].fillna(data['occupancy_rate'].mode()[0]) # fill missing values with mode\n",
    "    data['review_score_average'] = (data['review_scores_rating'] + data['review_scores_accuracy'] + \n",
    "                                    data['review_scores_cleanliness'] + data['review_scores_checkin'] + \n",
    "                                    data['review_scores_communication'] + data['review_scores_location'] + \n",
    "                                    data['review_scores_value']) / 7\n",
    "    data['review_score_average'] = data['review_score_average'].fillna(data['review_score_average'].mean()) # fill missing values with mode\n",
    "    data['property_room_type'] = data['property_type'] + \"_\" + data['room_type']\n",
    "    data['property_room_type'] = data['property_room_type'].fillna(data['property_room_type'].mode()[0]) # fill missing values with mode\n",
    "    mask = np.isinf(data['bedrooms_per_accommodat'])\n",
    "    # Use the mask to replace infinite values with corresponding 'bathrooms_text' values\n",
    "    data.loc[mask, 'bedrooms_per_accommodat'] = data.loc[mask, 'bathrooms_num']\n",
    "    data['bathrooms_per_accommodat'] = np.where(data['accommodates'] == 0, 0, data['bathrooms_num'] / data['accommodates'])\n",
    "    data['bedroom_bathroom_ratio'] = np.where(data['bathrooms_num'] == 0, 0, data['bedrooms']  / data['bathrooms_num'])\n",
    "    data['amenities_per_guest'] = np.where(data['accommodates'] == 0, 0, data['num_amenities']  / data['accommodates'])\n",
    "    \n",
    "    # if 'city', 'country', 'distance_from_capital' not in data.columns:\n",
    "    if 'city' not in data.columns:\n",
    "        data['city'], data['country'], data['distance_from_capital'] = zip(*data.apply(lambda row: get_location_by_coordinates(row['latitude'], row['longitude']), axis=1)) \n",
    "    \n",
    "    return data\n",
    "\n",
    "def cap_outliers(X_train, X_test, columns):\n",
    "    quantiles = {}\n",
    "    for column in columns:\n",
    "        quantiles[column] = X_train[column].quantile(0.99)\n",
    "    for column, quantile in quantiles.items():\n",
    "        X_train.loc[X_train[column] > quantile, column] = quantile # cap values in train set\n",
    "        X_test.loc[X_test[column] > quantile, column] = quantile # cap values in test set\n",
    "    return X_train, X_test\n",
    "\n",
    "def random_imputation(data, feature):\n",
    "    number_of_missing_values = data[feature].isnull().sum()\n",
    "    \n",
    "    if number_of_missing_values > 0:\n",
    "        # Generate random values from the non-missing values of 'feature'\n",
    "        random_sample = data[feature].dropna().sample(number_of_missing_values, random_state=0)\n",
    "        \n",
    "        # pandas needs to have the same index in order to merge datasets\n",
    "        random_sample.index = data[data[feature].isnull()].index\n",
    "        \n",
    "        # Replace the missing values with the random sample\n",
    "        data.loc[data[feature].isnull(), feature] = random_sample\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_missing_values(X_train, X_test, columns):\n",
    "    # Fill missing values in train and test sets\n",
    "    X_train = fill_na_with_host_specific_values(X_train, columns)\n",
    "    X_test = fill_na_with_host_specific_values(X_test, columns)\n",
    "\n",
    "    # Impute missing values in train and test sets\n",
    "    mice_cols = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month','number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d', 'bedrooms', 'beds']\n",
    "    X_train, X_test = impute_missing_values_mice_train_test(X_train, X_test, mice_cols, random_state=2023, max_iter=20)\n",
    "\n",
    "    # Random imputation in train and test sets\n",
    "    X_train = random_imputation(X_train, 'first_review')\n",
    "    X_test = random_imputation(X_test, 'first_review')\n",
    "\n",
    "    # Fill missing values in specific columns of train and test sets\n",
    "    for column in ['bathrooms_text','host_response_time']:\n",
    "        X_train[column].fillna(X_train[column].mode()[0], inplace=True)\n",
    "        X_test[column].fillna(X_train[column].mode()[0], inplace=True) # note: we use mode from train set\n",
    "\n",
    "    # Another example of filling missing values\n",
    "    for column in ['minimum_nights_avg_ntm', 'minimum_minimum_nights', 'maximum_maximum_nights', 'minimum_maximum_nights', 'maximum_minimum_nights', 'maximum_nights_avg_ntm','host_is_superhost']:\n",
    "        X_train[column].fillna(X_train[column].median(), inplace=True)\n",
    "        X_test[column].fillna(X_train[column].median(), inplace=True) # note: we use median from train set\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def standardize(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "def normalize(X):\n",
    "    normalizer = Normalizer()\n",
    "    return normalizer.fit_transform(X)\n",
    "\n",
    "# Convert Categorical variables\n",
    "def create_dummy_df(df, cat_cols, categories_dict, dummy_na):\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            # Get full list of categories for this column\n",
    "            categories = categories_dict[col]\n",
    "\n",
    "            # Create dummy variables, ensuring all categories are included\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)\n",
    "            dummies = dummies.reindex(columns=[col+'_'+str(category) for category in categories], fill_value=0)\n",
    "\n",
    "            # Add dummy variables to df and drop original column\n",
    "            df = pd.concat([df.drop(col, axis=1), dummies], axis=1)\n",
    "        except:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_function(X_train, X_test,transform_numeric='yeo-johnson', do_feature_eng=True, do_handle_missing=True, remove_missing=False,do_balance = False):\n",
    "    print('Total number of missing values before imputation in train set: ', X_train.isnull().sum().sum())\n",
    "    y_train = X_train['expensive']\n",
    "    X_train = X_train.drop('expensive', axis=1)\n",
    "\n",
    "    # transform columns to datetime\n",
    "    date_columns = ['last_review', 'first_review','host_since']  # Update this list based on your DataFrame\n",
    "    for column in date_columns:\n",
    "        X_train[column] = pd.to_datetime(X_train[column], format = '%d/%m/%Y')\n",
    "        X_test[column] = pd.to_datetime(X_test[column], format = '%d/%m/%Y')\n",
    "\n",
    "    # Replace '%' and convert to numeric\n",
    "    percentage_columns = ['host_acceptance_rate', 'host_response_rate']\n",
    "    for column in percentage_columns:\n",
    "        X_train[column] = X_train[column].str.replace('%', '').astype(float)\n",
    "        X_test[column] = X_test[column].str.replace('%', '').astype(float)\n",
    "\n",
    "    # Handle binary columns\n",
    "    binary_columns = X_train.columns[(X_train.nunique() == 2) & (X_train.dtypes == 'object')].tolist()\n",
    "    X_train[binary_columns] = X_train[binary_columns].apply(lambda x: x.map({'t': 1, 'f': 0}))\n",
    "    X_test[binary_columns] = X_test[binary_columns].apply(lambda x: x.map({'t': 1, 'f': 0}))\n",
    "\n",
    "    # Cap outliers\n",
    "    X_train, X_test = cap_outliers(X_train, X_test, ['minimum_nights','maximum_nights','minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm','maximum_nights_avg_ntm'])\n",
    "\n",
    "    # Handle missing values\n",
    "    if do_handle_missing:\n",
    "        X_train, X_test = handle_missing_values(X_train, X_test, ['host_response_rate', 'host_acceptance_rate', 'license', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month','number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d', 'bedrooms', 'beds', 'first_review', 'last_review', 'bathrooms_text','host_response_time', 'minimum_nights_avg_ntm', 'minimum_minimum_nights', 'maximum_maximum_nights', 'minimum_maximum_nights', 'maximum_minimum_nights', 'maximum_nights_avg_ntm','host_is_superhost'])\n",
    "    else:\n",
    "        # Filling missing categorical & binary columns with mode and numeric columns with mean\n",
    "        for col in X_train.columns:\n",
    "            if X_train[col].dtype == 'object':\n",
    "                X_train[col].fillna(X_train[col].mode()[0], inplace=True)\n",
    "                X_test[col].fillna(X_train[col].mode()[0], inplace=True) # using mode from training set\n",
    "            else:\n",
    "                X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
    "                X_test[col].fillna(X_train[col].mean(), inplace=True) # using mean from training set\n",
    "\n",
    "    print('Total number of missing values after imputation in train set: ', X_train.isnull().sum().sum())\n",
    "\n",
    "    # Feature engineering\n",
    "    if do_feature_eng:\n",
    "        X_train = create_new_features(X_train)\n",
    "        X_test = create_new_features(X_test)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    columns_to_remove = ['id','host_id','host_since','latitude','longitude','amenities','license','last_review','first_review','bathrooms_text', 'amenities']\n",
    "    X_train = X_train.drop(columns_to_remove, axis=1)\n",
    "    X_test = X_test.drop(columns_to_remove, axis=1)\n",
    "\n",
    "    # Drop rows with missing values in train and test sets\n",
    "    if remove_missing:\n",
    "        X_train = X_train.dropna()\n",
    "        X_test = X_test.dropna()\n",
    "\n",
    "    # Print all columns with infinite values in train set or test set\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    print('Infinite values in train set: ', numeric_cols[X_train[numeric_cols].apply(lambda x:np.any(np.isinf(x)))])\n",
    "    numeric_cols_test = X_test.select_dtypes(include=[np.number]).columns\n",
    "    print('Infinite values in test set: ', numeric_cols_test[X_test[numeric_cols_test].apply(lambda x:np.any(np.isinf(x)))])\n",
    "\n",
    "    if transform_numeric:\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        if transform_numeric == 'normalize':\n",
    "            X_train[numeric_cols] = normalize(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = normalize(X_test[numeric_cols])\n",
    "        elif transform_numeric == 'standardize':\n",
    "            X_train[numeric_cols] = standardize(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = standardize(X_test[numeric_cols])\n",
    "        elif transform_numeric == 'yeo-johnson':\n",
    "            X_train[numeric_cols] = yeo_johnson_transformer.fit_transform(standardize(X_train[numeric_cols]))\n",
    "            X_test[numeric_cols] = yeo_johnson_transformer.transform(standardize(X_test[numeric_cols]))\n",
    "\n",
    "    # Encoding categorical features\n",
    "    \n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Combine train and test data to get full list of categories\n",
    "    full_data = pd.concat([X_train, X_test])\n",
    "    # Create dictionary mapping each categorical column to its full list of categories\n",
    "    categories_dict = {col: full_data[col].dropna().unique() for col in categorical_cols}\n",
    "    \n",
    "    X_train = create_dummy_df(X_train, categorical_cols, categories_dict, dummy_na=False)\n",
    "    X_test = create_dummy_df(X_test, categorical_cols, categories_dict, dummy_na=False)\n",
    "    column_names = X_train.columns.tolist()\n",
    "    \n",
    "    if do_balance:\n",
    "        smote = SMOTE(random_state=2023)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "    return X_train,y_train, X_test,column_names\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def custom_cross_val_score(model, X_train, y_train, cv=20,do_lda = False):\n",
    "    kf = StratifiedKFold(n_splits=cv, random_state=2023, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
    "        y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        if do_lda:\n",
    "            lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "            X_train_cv = lda.fit_transform(X_train_cv, y_train_cv)\n",
    "            X_test_cv = lda.transform(X_test_cv)\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_proba_cv = model.predict_proba(X_test_cv)[:, 1]  # keep probabilities for the positive outcome only\n",
    "        score = roc_auc_score(y_test_cv, y_pred_proba_cv)\n",
    "        scores.append(score)\n",
    "    \n",
    "    print(\"Cross-validation scores: \", scores)\n",
    "    print(\"Mean cross-validation ROC AUC score: \", np.mean(scores))\n",
    "\n",
    "    \n",
    "    return scores,np.mean(scores) # return mean of CV scores and test score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values before imputation in train set:  8362\n",
      "Total number of missing values after imputation in train set:  0\n",
      "Infinite values in train set:  Index([], dtype='object')\n",
      "Infinite values in test set:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "x_test_ids = X_test['id']\n",
    "\n",
    "X_train, y_train, X_test,col_names = preprocess_function(X_train, X_test,do_balance=True,transform_numeric='yeo-johnson')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top correlations features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuval\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# Calculate Spearman correlation for each feature\n",
    "correlations = {}\n",
    "\n",
    "for column in X_train.columns:\n",
    "    correlation, _ = spearmanr(X_train[column], y_train)\n",
    "    correlations[column] = correlation\n",
    "    \n",
    "n = 60  # Specify the number of top features you want to retrieve    \n",
    "# Sort the correlations by absolute values in descending order\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Get the names of the top n features\n",
    "top_features = [feature for feature, _ in sorted_correlations[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9692, 144), (9692, 274))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Define the random seed\n",
    "random_seed = 2023\n",
    "\n",
    "\n",
    "# First, train the Lasso model and select the features\n",
    "lasso = LassoCV(random_state=random_seed)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_mask = lasso.coef_ != 0 # remove features with zero coefficient (i.e. remove features that the Lasso model has excluded)\n",
    "\n",
    "# Apply the mask to the train and test data\n",
    "X_train_lasso = X_train.loc[:, lasso_mask]\n",
    "X_test_lasso = X_test.loc[:, lasso_mask]\n",
    "\n",
    "\n",
    "# Then, train the ExtraTreesClassifier on the reduced datasets\n",
    "model = ExtraTreesClassifier(random_state=random_seed)\n",
    "model.fit(X_train_lasso, y_train)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_lasso.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "top_n = 40\n",
    "\n",
    "# Get the names of the top N features\n",
    "top_n_features = importance_df['Feature'].iloc[:top_n]\n",
    "\n",
    "\n",
    "X_train_selected = X_train_lasso[top_n_features]\n",
    "X_test_selected = X_test_lasso[top_n_features]\n",
    "\n",
    "X_train_lasso.shape,X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used two feature selection methods:\n",
    "\n",
    "LassoCV: This is a linear model trained with L1 prior as a regularizer which tends to make coefficients of less important features exactly zero. This property allows it to perform feature selection and makes the model more interpretable. It's especially effective for high-dimensional datasets.\n",
    "\n",
    "ExtraTreesClassifier: This is a tree-based model that fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control overfitting. The model's feature importances attribute is then used to select the most important features. The logic behind this is that the features that are selected more often across different trees are usually more important for the prediction task.\n",
    "\n",
    "Those feature selection methods give us the best auc-roc score in the cross-validation steps (from the other methods we test in the report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and export prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do pca  for 95% variance\n",
    "pca_13d= PCA(n_components=13) \n",
    "pca_13d.fit(X_train_selected)\n",
    "\n",
    "X_train_13d = pca_13d.transform(X_train_selected)\n",
    "X_test_13d = pca_13d.transform(X_test_selected)\n",
    "\n",
    "pca_13d.plot_scree()\n",
    "\n",
    "# check auc-roc score with cross validation\n",
    "model = myLogisticRegression()\n",
    "# custom_cross_val_score(model, X_train_50d , y_train, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well choose to reduce the dimensions to 13 using PCA in order to gain more that 90% of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myLogisticRegression()\n",
    "model.fit(X_train_13d, y_train)\n",
    "y_test_proba = model.predict_proba(X_test_13d)\n",
    "y_test_proba = y_test_proba[:, 1]  # keep probabilities for the positive outcome only\n",
    "prediction_df = pd.DataFrame({'id': x_test_ids, 'expensive': y_test_proba})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>expensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6760</td>\n",
       "      <td>0.232440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6761</td>\n",
       "      <td>0.871051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6762</td>\n",
       "      <td>0.872005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6763</td>\n",
       "      <td>0.214826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6764</td>\n",
       "      <td>0.869246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>9782</td>\n",
       "      <td>0.850720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>9783</td>\n",
       "      <td>0.889504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>9784</td>\n",
       "      <td>0.885722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>9785</td>\n",
       "      <td>0.206005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>9786</td>\n",
       "      <td>0.243866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3027 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  expensive\n",
       "0     6760   0.232440\n",
       "1     6761   0.871051\n",
       "2     6762   0.872005\n",
       "3     6763   0.214826\n",
       "4     6764   0.869246\n",
       "...    ...        ...\n",
       "3022  9782   0.850720\n",
       "3023  9783   0.889504\n",
       "3024  9784   0.885722\n",
       "3025  9785   0.206005\n",
       "3026  9786   0.243866\n",
       "\n",
       "[3027 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('data/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
